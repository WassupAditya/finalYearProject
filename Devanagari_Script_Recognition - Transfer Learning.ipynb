{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d09805c",
   "metadata": {
    "id": "5d09805c"
   },
   "source": [
    "### Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sYEGc9fCcoTE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sYEGc9fCcoTE",
    "outputId": "6f4fcdc0-5da9-420e-9ac1-1be46fe6ddec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras_preprocessing\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 0.0/42.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.6/42.6 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\sndes\\anaconda3\\lib\\site-packages (from keras_preprocessing) (1.24.3)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\sndes\\anaconda3\\lib\\site-packages (from keras_preprocessing) (1.16.0)\n",
      "Installing collected packages: keras_preprocessing\n",
      "Successfully installed keras_preprocessing-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install keras_preprocessing\n",
    "# pip install keras\n",
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189b315c",
   "metadata": {
    "id": "189b315c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense,Conv2D, MaxPool2D,Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras_preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.applications import ResNet152V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f6cc2a",
   "metadata": {
    "id": "f6f6cc2a"
   },
   "outputs": [],
   "source": [
    "img=load_img(r\"C:\\Users\\sndes\\JupyterProjects\\FinalProject\\Fin_DS\\train\\क\\5_क.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a9e813",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3a9e813",
    "outputId": "ef0afbc0-b022-445a-959e-8484489b6b42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b0711ac",
   "metadata": {
    "id": "1b0711ac"
   },
   "outputs": [],
   "source": [
    "img=img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97bc009d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "97bc009d",
    "outputId": "da77685e-c98f-439b-ab73-2bb4ca59d6db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c9438ba510>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "001113a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "001113a8",
    "outputId": "3f8e9971-21e5-4e62-8fd3-709d51c1a0fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f3f5a",
   "metadata": {
    "id": "0d0f3f5a"
   },
   "source": [
    "### Build the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e941588",
   "metadata": {
    "id": "0e941588"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(32, input_shape=(64,64,3), kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32,kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(513,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9df347a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9df347a",
    "outputId": "af84f6ac-e65e-46ae-83ad-ebe8573ed660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 31, 31, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 29, 29, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 14, 14, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               3211776   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 513)               263169    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3485089 (13.29 MB)\n",
      "Trainable params: 3485089 (13.29 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a98284",
   "metadata": {
    "id": "a9a98284"
   },
   "source": [
    "### Visualize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d44c34e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 812
    },
    "id": "d44c34e3",
    "outputId": "3bfe9c7c-cf65-4b29-a132-207d23fe9e2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, show_dtype=True, show_layer_names=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db15cbef",
   "metadata": {
    "id": "db15cbef"
   },
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2c705c7",
   "metadata": {
    "id": "a2c705c7"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5a9e65",
   "metadata": {
    "id": "6c5a9e65"
   },
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9f3ee24",
   "metadata": {
    "id": "f9f3ee24"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef71e53e",
   "metadata": {
    "id": "ef71e53e"
   },
   "outputs": [],
   "source": [
    "train_datsgen=ImageDataGenerator(rescale=1/255,\n",
    "                                shear_range=0.2,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02552efd",
   "metadata": {
    "id": "02552efd"
   },
   "outputs": [],
   "source": [
    "test_datsgen=ImageDataGenerator(rescale=1/255,\n",
    "                               shear_range=0.2,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb924401",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb924401",
    "outputId": "e7fb4d8c-228a-4b3a-efd0-f656a93aaa86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22258 images belonging to 513 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set=train_datsgen.flow_from_directory(r\"C:\\Users\\sndes\\JupyterProjects\\FinalProject\\Fin_DS\\train\",\n",
    "                                              target_size=(224, 224),\n",
    "                                              class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15fcef3c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15fcef3c",
    "outputId": "bec4aa7b-ce7d-4c25-db1c-30fcd311adb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6022 images belonging to 513 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set=train_datsgen.flow_from_directory(r\"C:\\Users\\sndes\\JupyterProjects\\FinalProject\\Fin_DS\\val\",\n",
    "                                              target_size=(224, 224),\n",
    "                                              class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bfb590",
   "metadata": {
    "id": "81bfb590"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c90ca3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c90ca3a",
    "outputId": "a88f63ba-62dd-4336-d27d-56095196c06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "696/696 [==============================] - 255s 365ms/step - loss: 6.2172 - accuracy: 0.0034\n",
      "Epoch 2/100\n",
      "696/696 [==============================] - 68s 98ms/step - loss: 5.4054 - accuracy: 0.0509\n",
      "Epoch 3/100\n",
      "696/696 [==============================] - 68s 97ms/step - loss: 4.0653 - accuracy: 0.1917\n",
      "Epoch 4/100\n",
      "696/696 [==============================] - 68s 97ms/step - loss: 3.0250 - accuracy: 0.3488\n",
      "Epoch 5/100\n",
      "696/696 [==============================] - 68s 97ms/step - loss: 2.3113 - accuracy: 0.4832\n",
      "Epoch 6/100\n",
      "696/696 [==============================] - 69s 99ms/step - loss: 1.8226 - accuracy: 0.5737\n",
      "Epoch 7/100\n",
      "696/696 [==============================] - 68s 97ms/step - loss: 1.4595 - accuracy: 0.6503\n",
      "Epoch 8/100\n",
      "696/696 [==============================] - 68s 98ms/step - loss: 1.2315 - accuracy: 0.6954\n",
      "Epoch 9/100\n",
      "696/696 [==============================] - 68s 97ms/step - loss: 1.0366 - accuracy: 0.7341\n",
      "Epoch 10/100\n",
      "696/696 [==============================] - 67s 96ms/step - loss: 0.8884 - accuracy: 0.7729\n",
      "Epoch 11/100\n",
      "696/696 [==============================] - 66s 95ms/step - loss: 0.7904 - accuracy: 0.7936\n",
      "Epoch 12/100\n",
      "696/696 [==============================] - 67s 97ms/step - loss: 0.7082 - accuracy: 0.8112\n",
      "Epoch 13/100\n",
      "696/696 [==============================] - 68s 98ms/step - loss: 0.6317 - accuracy: 0.8297\n",
      "Epoch 14/100\n",
      "696/696 [==============================] - 69s 99ms/step - loss: 0.5778 - accuracy: 0.8469\n",
      "Epoch 15/100\n",
      "696/696 [==============================] - 67s 96ms/step - loss: 0.5137 - accuracy: 0.8601\n",
      "Epoch 16/100\n",
      "696/696 [==============================] - 67s 96ms/step - loss: 0.4909 - accuracy: 0.8661\n",
      "Epoch 17/100\n",
      "696/696 [==============================] - 70s 100ms/step - loss: 0.4482 - accuracy: 0.8742\n",
      "Epoch 18/100\n",
      "696/696 [==============================] - 68s 98ms/step - loss: 0.4162 - accuracy: 0.8853\n",
      "Epoch 19/100\n",
      "696/696 [==============================] - 70s 101ms/step - loss: 0.3859 - accuracy: 0.8930\n",
      "Epoch 20/100\n",
      "696/696 [==============================] - 68s 98ms/step - loss: 0.3782 - accuracy: 0.8937\n",
      "Epoch 21/100\n",
      "696/696 [==============================] - 67s 96ms/step - loss: 0.3517 - accuracy: 0.9013\n",
      "Epoch 22/100\n",
      "696/696 [==============================] - 67s 96ms/step - loss: 0.3340 - accuracy: 0.9065\n",
      "Epoch 23/100\n",
      "696/696 [==============================] - 67s 96ms/step - loss: 0.3259 - accuracy: 0.9097\n",
      "Epoch 24/100\n",
      "696/696 [==============================] - 68s 98ms/step - loss: 0.2988 - accuracy: 0.9145\n",
      "Epoch 25/100\n",
      "696/696 [==============================] - 67s 96ms/step - loss: 0.2919 - accuracy: 0.9177\n",
      "Epoch 26/100\n",
      "696/696 [==============================] - 68s 97ms/step - loss: 0.2796 - accuracy: 0.9189\n",
      "Epoch 27/100\n",
      "696/696 [==============================] - 69s 99ms/step - loss: 0.2620 - accuracy: 0.9245\n",
      "Epoch 28/100\n",
      "696/696 [==============================] - 68s 97ms/step - loss: 0.2543 - accuracy: 0.9278\n",
      "Epoch 29/100\n",
      "696/696 [==============================] - 66s 96ms/step - loss: 0.2380 - accuracy: 0.9309\n",
      "Epoch 30/100\n",
      "696/696 [==============================] - 67s 97ms/step - loss: 0.2345 - accuracy: 0.9346\n",
      "Epoch 31/100\n",
      "696/696 [==============================] - 67s 96ms/step - loss: 0.2289 - accuracy: 0.9339\n",
      "Epoch 32/100\n",
      "696/696 [==============================] - 66s 95ms/step - loss: 0.2162 - accuracy: 0.9394\n",
      "Epoch 33/100\n",
      "696/696 [==============================] - 67s 96ms/step - loss: 0.2113 - accuracy: 0.9384\n",
      "Epoch 34/100\n",
      "696/696 [==============================] - 67s 96ms/step - loss: 0.2006 - accuracy: 0.9417\n",
      "Epoch 35/100\n",
      "696/696 [==============================] - 69s 98ms/step - loss: 0.2111 - accuracy: 0.9406\n",
      "Epoch 36/100\n",
      "696/696 [==============================] - 66s 95ms/step - loss: 0.1916 - accuracy: 0.9442\n",
      "Epoch 37/100\n",
      "696/696 [==============================] - 66s 95ms/step - loss: 0.1889 - accuracy: 0.9449\n",
      "Epoch 38/100\n",
      "696/696 [==============================] - 64s 92ms/step - loss: 0.1838 - accuracy: 0.9479\n",
      "Epoch 39/100\n",
      "696/696 [==============================] - 55s 80ms/step - loss: 0.1768 - accuracy: 0.9483\n",
      "Epoch 40/100\n",
      "696/696 [==============================] - 52s 74ms/step - loss: 0.1761 - accuracy: 0.9504\n",
      "Epoch 41/100\n",
      "696/696 [==============================] - 60s 86ms/step - loss: 0.1681 - accuracy: 0.9500\n",
      "Epoch 42/100\n",
      "696/696 [==============================] - 61s 88ms/step - loss: 0.1624 - accuracy: 0.9525\n",
      "Epoch 43/100\n",
      "696/696 [==============================] - 58s 83ms/step - loss: 0.1682 - accuracy: 0.9504\n",
      "Epoch 44/100\n",
      "696/696 [==============================] - 58s 84ms/step - loss: 0.1560 - accuracy: 0.9545\n",
      "Epoch 45/100\n",
      "696/696 [==============================] - 59s 85ms/step - loss: 0.1689 - accuracy: 0.9514\n",
      "Epoch 46/100\n",
      "696/696 [==============================] - 62s 89ms/step - loss: 0.1562 - accuracy: 0.9548\n",
      "Epoch 47/100\n",
      "696/696 [==============================] - 57s 82ms/step - loss: 0.1492 - accuracy: 0.9565\n",
      "Epoch 48/100\n",
      "696/696 [==============================] - 57s 82ms/step - loss: 0.1417 - accuracy: 0.9570\n",
      "Epoch 49/100\n",
      "696/696 [==============================] - 56s 81ms/step - loss: 0.1467 - accuracy: 0.9575\n",
      "Epoch 50/100\n",
      "696/696 [==============================] - 55s 79ms/step - loss: 0.1343 - accuracy: 0.9606\n",
      "Epoch 51/100\n",
      "696/696 [==============================] - 56s 80ms/step - loss: 0.1389 - accuracy: 0.9596\n",
      "Epoch 52/100\n",
      "696/696 [==============================] - 55s 79ms/step - loss: 0.1296 - accuracy: 0.9625\n",
      "Epoch 53/100\n",
      "696/696 [==============================] - 55s 79ms/step - loss: 0.1383 - accuracy: 0.9590\n",
      "Epoch 54/100\n",
      "696/696 [==============================] - 55s 79ms/step - loss: 0.1283 - accuracy: 0.9615\n",
      "Epoch 55/100\n",
      "696/696 [==============================] - 55s 79ms/step - loss: 0.1291 - accuracy: 0.9615\n",
      "Epoch 56/100\n",
      "696/696 [==============================] - 55s 79ms/step - loss: 0.1182 - accuracy: 0.9650\n",
      "Epoch 57/100\n",
      "696/696 [==============================] - 55s 79ms/step - loss: 0.1254 - accuracy: 0.9628\n",
      "Epoch 58/100\n",
      "696/696 [==============================] - 55s 79ms/step - loss: 0.1283 - accuracy: 0.9610\n",
      "Epoch 59/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.1228 - accuracy: 0.9633\n",
      "Epoch 60/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.1111 - accuracy: 0.9671\n",
      "Epoch 61/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.1169 - accuracy: 0.9655\n",
      "Epoch 62/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.0967 - accuracy: 0.9707\n",
      "Epoch 63/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.1123 - accuracy: 0.9669\n",
      "Epoch 64/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.1093 - accuracy: 0.9674\n",
      "Epoch 65/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.1105 - accuracy: 0.9675\n",
      "Epoch 66/100\n",
      "696/696 [==============================] - 53s 77ms/step - loss: 0.1097 - accuracy: 0.9673\n",
      "Epoch 67/100\n",
      "696/696 [==============================] - 53s 77ms/step - loss: 0.1100 - accuracy: 0.9680\n",
      "Epoch 68/100\n",
      "696/696 [==============================] - 53s 77ms/step - loss: 0.1107 - accuracy: 0.9681\n",
      "Epoch 69/100\n",
      "696/696 [==============================] - 53s 77ms/step - loss: 0.1001 - accuracy: 0.9702\n",
      "Epoch 70/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.0955 - accuracy: 0.9709\n",
      "Epoch 71/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.1033 - accuracy: 0.9709\n",
      "Epoch 72/100\n",
      "696/696 [==============================] - 54s 78ms/step - loss: 0.1035 - accuracy: 0.9691\n",
      "Epoch 73/100\n",
      "696/696 [==============================] - 55s 79ms/step - loss: 0.0918 - accuracy: 0.9722\n",
      "Epoch 74/100\n",
      "696/696 [==============================] - 54s 78ms/step - loss: 0.0929 - accuracy: 0.9725\n",
      "Epoch 75/100\n",
      "696/696 [==============================] - 55s 79ms/step - loss: 0.0937 - accuracy: 0.9729\n",
      "Epoch 76/100\n",
      "696/696 [==============================] - 54s 78ms/step - loss: 0.0888 - accuracy: 0.9735\n",
      "Epoch 77/100\n",
      "696/696 [==============================] - 55s 78ms/step - loss: 0.0955 - accuracy: 0.9728\n",
      "Epoch 78/100\n",
      "696/696 [==============================] - 54s 78ms/step - loss: 0.0928 - accuracy: 0.9737\n",
      "Epoch 79/100\n",
      "696/696 [==============================] - 55s 78ms/step - loss: 0.0906 - accuracy: 0.9742\n",
      "Epoch 80/100\n",
      "696/696 [==============================] - 55s 78ms/step - loss: 0.0871 - accuracy: 0.9748\n",
      "Epoch 81/100\n",
      "696/696 [==============================] - 54s 78ms/step - loss: 0.0866 - accuracy: 0.9741\n",
      "Epoch 82/100\n",
      "696/696 [==============================] - 55s 79ms/step - loss: 0.0908 - accuracy: 0.9740\n",
      "Epoch 83/100\n",
      "696/696 [==============================] - 54s 78ms/step - loss: 0.0920 - accuracy: 0.9729\n",
      "Epoch 84/100\n",
      "696/696 [==============================] - 53s 75ms/step - loss: 0.0831 - accuracy: 0.9748\n",
      "Epoch 85/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.0889 - accuracy: 0.9731\n",
      "Epoch 86/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.0836 - accuracy: 0.9754\n",
      "Epoch 87/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.0872 - accuracy: 0.9744\n",
      "Epoch 88/100\n",
      "696/696 [==============================] - 53s 77ms/step - loss: 0.0779 - accuracy: 0.9759\n",
      "Epoch 89/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.0895 - accuracy: 0.9747\n",
      "Epoch 90/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.0737 - accuracy: 0.9774\n",
      "Epoch 91/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.0763 - accuracy: 0.9766\n",
      "Epoch 92/100\n",
      "696/696 [==============================] - 53s 77ms/step - loss: 0.0769 - accuracy: 0.9761\n",
      "Epoch 93/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.0822 - accuracy: 0.9763\n",
      "Epoch 94/100\n",
      "696/696 [==============================] - 53s 77ms/step - loss: 0.0796 - accuracy: 0.9761\n",
      "Epoch 95/100\n",
      "696/696 [==============================] - 53s 77ms/step - loss: 0.0733 - accuracy: 0.9785\n",
      "Epoch 96/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.0755 - accuracy: 0.9780\n",
      "Epoch 97/100\n",
      "696/696 [==============================] - 52s 75ms/step - loss: 0.0778 - accuracy: 0.9780\n",
      "Epoch 98/100\n",
      "696/696 [==============================] - 51s 74ms/step - loss: 0.0773 - accuracy: 0.9779\n",
      "Epoch 99/100\n",
      "696/696 [==============================] - 52s 75ms/step - loss: 0.0787 - accuracy: 0.9771\n",
      "Epoch 100/100\n",
      "696/696 [==============================] - 53s 76ms/step - loss: 0.0774 - accuracy: 0.9764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1c944200e50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_set, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c533f745",
   "metadata": {
    "id": "c533f745"
   },
   "source": [
    "### Evaluate on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f34ceeb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f34ceeb8",
    "outputId": "d38c08d3-cd13-4e67-8547-438f2fb093e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 40s 211ms/step - loss: 1.3656 - accuracy: 0.8401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3655879497528076, 0.8400863409042358]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c77be26",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a3b3f1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " densenet121 (Functional)    (None, 2, 2, 1024)        7037504   \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 1024)              0         \n",
      " 0 (GlobalAveragePooling2D)                                      \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 513)               263169    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7825473 (29.85 MB)\n",
      "Trainable params: 787969 (3.01 MB)\n",
      "Non-trainable params: 7037504 (26.85 MB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22258 images belonging to 513 classes.\n",
      "Found 6022 images belonging to 513 classes.\n",
      "Epoch 1/50\n",
      "696/696 [==============================] - 212s 299ms/step - loss: 5.8228 - accuracy: 0.0136 - val_loss: 5.1764 - val_accuracy: 0.0382\n",
      "Epoch 2/50\n",
      "696/696 [==============================] - 215s 309ms/step - loss: 5.0143 - accuracy: 0.0466 - val_loss: 4.6737 - val_accuracy: 0.0676\n",
      "Epoch 3/50\n",
      "696/696 [==============================] - 216s 310ms/step - loss: 4.6934 - accuracy: 0.0731 - val_loss: 4.4597 - val_accuracy: 0.0844\n",
      "Epoch 4/50\n",
      "669/696 [===========================>..] - ETA: 6s - loss: 4.4916 - accuracy: 0.0868"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 66\u001b[0m\n\u001b[0;32m     59\u001b[0m test_set \u001b[38;5;241m=\u001b[39m test_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msndes\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mJupyterProjects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFinalProject\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFin_DS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     61\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m),\n\u001b[0;32m     62\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     63\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(training_set, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39mtest_set)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Evaluate on unseen data\u001b[39;00m\n\u001b[0;32m     69\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(test_set)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    868\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[0;32m    869\u001b[0m   )\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mflat_call(args)\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    256\u001b[0m     )\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1480\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1481\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1482\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1483\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1484\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1485\u001b[0m   )\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.applications import DenseNet121\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create a DenseNet121 base model\n",
    "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the pre-trained DenseNet121 model\n",
    "model.add(base_model)\n",
    "\n",
    "# Add Global Average Pooling 2D layer\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "# Add your own classifier on top\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(513, activation='softmax'))\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Use data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2)\n",
    "\n",
    "# No data augmentation for validation/test\n",
    "test_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Load and augment the training data\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\sndes\\JupyterProjects\\FinalProject\\Fin_DS\\train\",\n",
    "    target_size=(64, 64),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Load the validation data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\sndes\\JupyterProjects\\FinalProject\\Fin_DS\\val\",\n",
    "    target_size=(64, 64),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Train the model\n",
    "model.fit(training_set, epochs=50, validation_data=test_set)\n",
    "\n",
    "# Evaluate on unseen data\n",
    "model.evaluate(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0b0dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ResNet152V2 model pre-trained on ImageNet data\n",
    "base_model = ResNet152V2(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "\n",
    "# Freeze the convolutional layers of the ResNet152V2 model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71187074",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add the ResNet152V2 base model\n",
    "model.add(base_model)\n",
    "\n",
    "# Add your own layers on top of the ResNet152V2 model with 'SAME' padding\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(513, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bbbabd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet152v2 (Functional)    (None, 2, 2, 2048)        58331648  \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 2, 2, 32)          589856    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 1, 1, 32)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 1, 32)          9248      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 1, 1, 32)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 513)               263169    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 59210817 (225.87 MB)\n",
      "Trainable params: 879169 (3.35 MB)\n",
      "Non-trainable params: 58331648 (222.52 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfee54ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1366db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ImageDataGenerators\n",
    "train_datagen = ImageDataGenerator(rescale=1/255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1/255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18367bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22258 images belonging to 513 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data using ImageDataGenerators\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\sndes\\JupyterProjects\\FinalProject\\Fin_DS\\train\",\n",
    "    target_size=(64, 64),\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a08a4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6022 images belonging to 513 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\sndes\\JupyterProjects\\FinalProject\\Fin_DS\\val\",\n",
    "    target_size=(64, 64),\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37ccab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "696/696 [==============================] - 263s 378ms/step - loss: 2.4362 - accuracy: 0.3997\n",
      "Epoch 2/5\n",
      "696/696 [==============================] - 272s 390ms/step - loss: 2.4012 - accuracy: 0.4059\n",
      "Epoch 3/5\n",
      "696/696 [==============================] - 272s 391ms/step - loss: 2.4107 - accuracy: 0.4028\n",
      "Epoch 4/5\n",
      "696/696 [==============================] - 279s 401ms/step - loss: 2.3884 - accuracy: 0.4100\n",
      "Epoch 5/5\n",
      "696/696 [==============================] - 275s 394ms/step - loss: 2.3834 - accuracy: 0.4104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1be0aff2110>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(training_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01d3db9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 66s 349ms/step - loss: 2.7719 - accuracy: 0.3404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.7718911170959473, 0.3404184579849243]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8007a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess a test image\n",
    "test_image = load_img(r\"C:\\Users\\sndes\\JupyterProjects\\FinalProject\\Fin_DS\\val\\र्व\\1_र्व.jpg\", target_size=(64, 64))\n",
    "test_image = img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "# Get class indices\n",
    "indices = training_set.class_indices\n",
    "\n",
    "# Make predictions\n",
    "result = np.argmax(model.predict(test_image, verbose=False))\n",
    "print(list(indices.keys())[list(indices.values()).index(result)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423fe209",
   "metadata": {
    "id": "423fe209"
   },
   "source": [
    "### Prediction on sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e36876f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "8e36876f",
    "outputId": "fa773fbb-fd0e-4e0c-e3e6-522df3c04dc0"
   },
   "outputs": [],
   "source": [
    "test_image=load_img(r\"C:\\Users\\sndes\\JupyterProjects\\FinalProject\\Fin_DS\\val\\र्व\\1_र्व.jpg\",target_size=(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a4f1e29",
   "metadata": {
    "id": "7a4f1e29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c946cf1c10>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGfCAYAAAD22G0fAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqLUlEQVR4nO3df2yV133H8Y8JcMHGvoQfvtdWHOYmTppASBPIHJyssKV4YlnUCKlrS9pRTZqgJC1eNpE4SItTtXZGNUQnUk+wKSXqGP8ktFRrEzy1Md1QVkKDQqEiRLiJ03LrAOba/Mh1Cmd/RFzF+HwTjrnm2Jf3S7pScu7D43PuNXz9+Pnc7ylxzjkBABDBuNgTAABcvShCAIBoKEIAgGgoQgCAaChCAIBoKEIAgGgoQgCAaChCAIBoKEIAgGgoQgCAaMaP1Im/+93v6tvf/raOHj2q2bNna8OGDfqTP/mTj/1z58+f1+9+9zuVl5erpKRkpKYHABghzjn19/erurpa48Z9zLWOGwHbtm1zEyZMcJs3b3YHDx50q1evdmVlZe6tt9762D/b3d3tJPHgwYMHjzH+6O7u/th/80ucK3wD0/r6et15551qb2/Pj91yyy168MEH1dbW9pF/NpvNaurUqeru7lZFRUWhpwYAGGF9fX2qqanRyZMnlUwmP/LYgv86bmBgQHv37tXjjz8+aLyxsVG7d+8ecnwul1Mul8v/f39/vySpoqKCIgQAY9il3FIpeDDh2LFjOnfunFKp1KDxVCqlTCYz5Pi2tjYlk8n8o6amptBTAgCMUiOWjru4AjrnvFWxublZ2Ww2/+ju7h6pKQEARpmC/zpuxowZuuaaa4Zc9fT09Ay5OpKkRCKhRCJR6GkAAMaAgl8JTZw4UfPmzVNHR8eg8Y6ODjU0NBT6ywEAxrAR+ZzQo48+qi9/+cuaP3++FixYoE2bNuntt9/WypUrR+LLAQDGqBEpQp///Od1/PhxfeMb39DRo0c1Z84c/fjHP9asWbNG4ssBAMaoEfmc0OXo6+tTMplUNpslog0AY1DIv+P0jgMAREMRAgBEQxECAERDEQIAREMRAgBEQxECAERDEQIAREMRAgBEQxECAERDEQIAREMRAgBEQxECAERDEQIAREMRAgBEQxECAERDEQIAREMRAgBEQxECAERDEQIAREMRAgBEQxECAERDEQIAREMRAgBEQxECAERDEQIAREMRAgBEQxECAERDEQIAREMRAgBEQxECAERDEQIAREMRAgBEQxECAERDEQIAREMRAgBEMz72BBDfuXPnvOPXXHONd/y9997zjk+aNOmSz//+++8HnSOXy3nHnXPe8YkTJw4ZGzfO/zPXmTNnvOOlpaXecZ+zZ896xydPnuwdt9Y/YcIE77jvNbTen1A9PT1DxiorKwtybuDjcCUEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIZ0HHT+/HnvuJW+shJsVkKspKTkks9hSSQSQcf71jQwMOA9trq62juezWa9476UnfUaWuk9KwV3+vRp73hZWZl33Cc0vehLwllpxND3Afg4XAkBAKKhCAEAoqEIAQCioQgBAKKhCAEAoglOx+3atUvf/va3tXfvXh09elTbt2/Xgw8+mH/eOaennnpKmzZtUm9vr+rr6/XMM89o9uzZhZw3CshKalkJKasHm3We8eMv/dvsxIkT3vGpU6d6x60E2w033DBkrLe395LnIdn903y91qzUmC8ZKNmvYWjKzseaS0h/O1JwuFKCr4ROnz6t22+/XRs3bvQ+v27dOq1fv14bN27Unj17lE6ntXjxYvX391/2ZAEAxSX4SmjJkiVasmSJ9znnnDZs2KC1a9dq6dKlkqQtW7YolUpp69atWrFixZA/k8vlBv3E3dfXFzolAMAYVdB7Ql1dXcpkMmpsbMyPJRIJLVy4ULt37/b+mba2NiWTyfyjpqamkFMCAIxiBS1CmUxGkpRKpQaNp1Kp/HMXa25uVjabzT+6u7sLOSUAwCg2Im17Lr4Z65wzb9AmEgluggLAVaqgRSidTkv64IqoqqoqP97T0zPk6gijR2h/MyvZZfHtImr1cZs2bVrQua0E2x/+8IdLPof1Q5AvBSf5e7CFJgytPm7Wjqu+H+Ks9836gS/kNQndbRcYroL+Oq62tlbpdFodHR35sYGBAXV2dqqhoaGQXwoAUASCr4ROnTqlN998M///XV1d2rdvn6ZNm6brr79eTU1Nam1tVV1dnerq6tTa2qrS0lItW7asoBMHAIx9wUXo1Vdf1Z/+6Z/m///RRx+VJC1fvlzf+973tGbNGp09e1arVq3Kf1h1586dKi8vL9ysAQBFocSFfBT7Cujr61MymVQ2m1VFRUXs6VwVrG8Ba3wk7wmF7Jsj2fdiCnFPKOR+jjUP60PapaWl3nHrnpBvPPSvrjUX3w+I3BPC5Qj5d5xN7RBcbKz2LxMnTvSO+/6Btv7RtlrrWIEF6zw+1vx8bWsk6fjx495xXxGyPmRtfc0pU6Z4xy2+dVoBBOv9tH4b4SvYVhGnCKHQaGAKAIiGIgQAiIYiBACIhiIEAIiGIgQAiIZ0HMwospUas8YtvuSYFdu89tprveNWEsyKNE+fPn3I2BtvvOE9NplMesdDkmDWvC2h8WrfXKz0ovVaHTt2zDvue61CIu7A5eBKCAAQDUUIABANRQgAEA1FCAAQDUUIABAN6TiYaTcrIWU1H7WacvqScNa5Q/uhWcf7+r6Fbphn8TUCtXreXX/99d5xK5FoNVN97733hoxZfemsjf6mTp3qHfexNt0DCo0rIQBANBQhAEA0FCEAQDQUIQBANBQhAEA0pONgGj/e/+1hjVs7rvrSd9Y5rOSdlQSz+rtZu6KGOH36tHfct0OptWupteOq1TsvZAvu0MRgSC+8U6dOecdDd4QFPg5XQgCAaChCAIBoKEIAgGgoQgCAaAgmwHTmzBnvuNWex7oh7mO17bFu2FuBBetrhmy8Z81lxowZ3nFfYMHaYM5az/nz573jVtueEOfOnfOOW+v0hUTKysqCvmYhggwh8/sovvcndD24crgSAgBEQxECAERDEQIAREMRAgBEQxECAERDOg5mKslKwVlCNkKzWs5Ym71ZrHScL9lnrcdKX3V3d3vHfa11ksmk91jfZnSS/VpZ7Yl875G1dmvcWmfIhnnW+xbazuf9998fMmZ9H1pJQmuOJOHGFq6EAADRUIQAANFQhAAA0VCEAADRUIQAANGQjkNBenNJdrrJlxyzNliz0ldWmsxK0/mScFZSzVq/1TsuhNULLnTzvpD3yOodZzl58uSQsXQ67T02m816x610YMgmfRMmTPAe60vSSYXrNYe4uBICAERDEQIAREMRAgBEQxECAERDEQIAREOMBHr33Xe94zNnzvSOF6I3l9XfzEpfWSk4a0fTkN5xVlLNEtJrzUpwWcdbfIk3q6ealTC0EolWEs7HSsFZrB1ufXO33ktrHMWBdxcAEA1FCAAQDUUIABANRQgAEE1QEWpra9Ndd92l8vJyVVZW6sEHH9ShQ4cGHeOcU0tLi6qrqzV58mQtWrRIBw4cKOikAQDFISgd19nZqYcfflh33XWX/vCHP2jt2rVqbGzUwYMH84mpdevWaf369fre976nm266Sd/85je1ePFiHTp0yNsvCvFZKThL6A6YvlSWldQK3aHT6jfmS/BZfcyseVt8feysRFohUnDWeOi5rZ5/vtfK2lU2lUp5x0N35/Udf+LECe+xoT38fOemn9zoFfTOvPjii4P+/9lnn1VlZaX27t2rT3/603LOacOGDVq7dq2WLl0qSdqyZYtSqZS2bt2qFStWFG7mAIAx77LuCV34TMe0adMkSV1dXcpkMmpsbMwfk0gktHDhQu3evdt7jlwup76+vkEPAMDVYdhFyDmnRx99VPfee6/mzJkjScpkMpKGXrKnUqn8cxdra2tTMpnMP2pqaoY7JQDAGDPsIvTII4/o9ddf13/+538Oee7iT8M758xPyDc3NyubzeYf1u+iAQDFZ1h36772ta9px44d2rVrl6677rr8+IX2H5lMRlVVVfnxnp4e84ZmIpEwN/5CXL7WN5J9s9m6qe4LIZw9e9Z7rNXmxWK18/Gd5+abb/Yee/jwYe94yI1/a+1WYCG0FU1oCMHHet98Qn8jEdr6yBcUCA0gWK+t770gmDB6Bf1NcM7pkUce0QsvvKCf/vSnqq2tHfR8bW2t0um0Ojo68mMDAwPq7OxUQ0NDYWYMACgaQT8ePPzww9q6dat++MMfqry8PH+fJ5lMavLkySopKVFTU5NaW1tVV1enuro6tba2qrS0VMuWLRuRBQAAxq6gItTe3i5JWrRo0aDxZ599Vl/5ylckSWvWrNHZs2e1atUq9fb2qr6+Xjt37uQzQgCAIYKKkPU72A8rKSlRS0uLWlpahjsnAMBVgt5xAIBoiIzATDaFJtWsVjy+djHWxnhW6xbrKtxq2+NL31mJPGsu1tf0bWpnfQShUMnP999/f8iYNT/r/bRaIr355ptDxm688UbvsdaHySsqKrzjVjufkLTayZMnveNTp071jpO2HVu4EgIAREMRAgBEQxECAERDEQIAREMRAgBEQzoOwX3JTp065R230le+tJLV8+3CtiAXsxJfvtSY9TWt1JS12Z2VeLuUz8tdYM3PYqX9fKz3zRq3+tvNnj17yJj1elsfOn/nnXe841Zqzve6WGu33geLLwUZmvTElcOVEAAgGooQACAaihAAIBqKEAAgGooQACAa0nFXgJUEO3/+vHfcl+SxzmElvkJ3RQ1hpeCOHz/uHZ8+ffqQMavPnNVrLDTB50t3WemwUL6EmJUYtBJfoam5SZMmDRnz9bCT7L5s1mvrE5oktPq4Wd/jIb3jrGSb9X6ShBtbuBICAERDEQIAREMRAgBEQxECAERDEQIAREM6roBCd520+FI/Vpoom816x5PJ5CV/PStlZaWMfEktyd651Lf+0JSVtbum5dprrx0y1tvb6z3WWo+V7PIl4az0ntWDzWKl0nzvkXWs1WstZJdTK41ppSutNGZIqjM0AWolDH3j1nuM+LgSAgBEQxECAERDEQIAREMRAgBEQzChgKwAQsgNYcm/aZp1rBVACAkbWG14rNY61rmtFjUh7WKscIPFuuFshRB8rPVYfK+LFUCYMWOGd9xq82PNxXdzPrRVkMX3/vhaLUl2a6aQjf4k6dixY0PGrO/DEydOeMetDRBD14+4uBICAERDEQIAREMRAgBEQxECAERDEQIAREM67gqw2otYiTdf+spqaWIlgazU2LhxQ3/usFJWVvsXS0hrFOtr+jaMk+yEXUiaztoEzXofUqmUd/z3v//9JX/Nt99+2zseuvGary2O9X0VmlTzOX36tHe8rKzMO261j7K+J6zUYMg5rPWTjhtbuBICAERDEQIAREMRAgBEQxECAERDEQIAREM6roCs9JW14ZnFl0qzNvayNhOzNh/z9VSz+s9Zm7pZ3n33Xe+4b6M6K8FkbQxo9RWzkmC+19DqhRdyDsk/dyupZaXgrLSjlQL0vZ/W/KzvQ+vcvvVbKTgrNWd9D1k99Xzrt77HrQ0QrSSlbz2hSU9cOVwJAQCioQgBAKKhCAEAoqEIAQCioQgBAKIhHVdAVvrKSnxZCTarl1nIOazUnG/319BeY1b6aubMmZd8jpMnT3rHfUm64fCl1ax5h/S8k/yvl7UTaeiuoFbKzpfustKL1u6sVuLNxzq3lWCzevhZ38shabXQXnC+RJ41b8THlRAAIBqKEAAgGooQACAaihAAIJqgYEJ7e7va29v1m9/8RpI0e/Zs/eM//qOWLFki6YMbtk899ZQ2bdqk3t5e1dfX65lnntHs2bMLPvGxxLeRnBQWQLDCDb6ggWQHFnw3ra15WOPWhnQhrVusAILVzsYKT1ihAt/N7EJtduZrf3P8+HHvsVYAwVqP1Z7IxwoxWAGEkE3grGCC9T1hBU1CNq+zwjShwRHr7xtGp6B367rrrtPTTz+tV199Va+++qr+7M/+TJ/97Gd14MABSdK6deu0fv16bdy4UXv27FE6ndbixYvN3k8AgKtbUBF64IEH9Bd/8Re66aabdNNNN+lb3/qWpkyZoldeeUXOOW3YsEFr167V0qVLNWfOHG3ZskVnzpzR1q1bR2r+AIAxbNjXrefOndO2bdt0+vRpLViwQF1dXcpkMmpsbMwfk0gktHDhQu3evds8Ty6XU19f36AHAODqEFyE9u/frylTpiiRSGjlypXavn27br31VmUyGUlSKpUadHwqlco/59PW1qZkMpl/1NTUhE4JADBGBRehm2++Wfv27dMrr7yir371q1q+fLkOHjyYf/7iT0I75z7y09HNzc3KZrP5R3d3d+iUAABjVHDbnokTJ+rGG2+UJM2fP1979uzRd77zHT322GOSpEwmo6qqqvzxPT09Q66OPiyRSBRNSw0rfRSSeJL8LVCsFJy1UZk1HpI0stZjpeCsZJvv/bWOtX5gCf0e8b2G1gZz1mtlHe9Ljlmbt1nrsVrrWOv0bUZ47bXXeo+1WHP0peOsFJy1qZ2VggvZMNBKdFrvj5WCK1QKElfGZWcZnXPK5XKqra1VOp1WR0dH/rmBgQF1dnaqoaHhcr8MAKAIBV0JPfHEE1qyZIlqamrU39+vbdu26eWXX9aLL76okpISNTU1qbW1VXV1daqrq1Nra6tKS0u1bNmykZo/AGAMCypCv//97/XlL39ZR48eVTKZ1Ny5c/Xiiy9q8eLFkqQ1a9bo7NmzWrVqVf7Dqjt37jT3ggcAXN1KXGgf/xHW19enZDKpbDZr3gcZrUI+kf5RCnE/YyTvCVnrGcl7QhMnTvSOWwrxGlrffyH3hKy/XjHuCVn3c0K2eAg9R8g9IYv1/oR0HMGVFfLvOP0tAADR8KNEAYX8dPdRfJuvWT85W1c21k/mIaxN+izW1/T9dG/9xG+9htY6rZ+Gfb3crJ/irSskq5eZb47pdNp7rPUZOWv9Vl++kKse68phJK94LIX4O8EVT3HjSggAEA1FCAAQDUUIABANRQgAEA1FCAAQDbGTArJ6Wfk+syLZqazQXnM+VkLKl76yPvcTmmCz+I63knehaT8rNehLk1mva09Pj3c85LNJVgrOOoe1c2khdla13nvra/rmGJqCA4aLKyEAQDQUIQBANBQhAEA0FCEAQDQUIQBANKTjCshKx1kpOF+POMmfHLN6eVnpq5AO5NY5rPSVtTWHlWx79913h4yFdka2kl1Wisv3elm94CzW1/Sx3nvrHP39/d5xa9da3/HW+2ClHUPSdPRrw5XClRAAIBqKEAAgGooQACAaihAAIBruPhZQ6FbGVssZ38320DYqhw4d8o7PnTt3yJh1w9pihQp8AQSL1YYndCt0a+6VlZWXfKwV+gjZIt16L61gghVAsPjCLda5rZCENQ7ExHclACAaihAAIBqKEAAgGooQACAaihAAIBrScQV08uRJ77hvgzUpLPEWujHezTff7B33pc9CN0Gz5mKlA0Nks1nvuNWixkrZvfXWW0PGrFZG1uZ9Fuv18rFaIlnzts5dWlp6yceeOHHCOz5jxgzveMg8aOeDQuNKCAAQDUUIABANRQgAEA1FCAAQDUUIABANUZcCslJwllOnTnnHfak5KwUXytcnLTTBZaXgrI3afMk2a0O/KVOmeMd9G/1JduLNl7Lr6+vzHpvL5bzj1np8X9N6f6wkYehr7mMl1UJScJL//bTeH9JxKDSuhAAA0VCEAADRUIQAANFQhAAA0VCEAADREHUpICs1Zo1bSTAfKzVl7RZq7VDqS95Zu4Ja87aSU77+ZhYrkWadI3TXWt+4dY6jR496x6uqqrzjIfOwdjO1knpW2s/Xyy00qWb1Npw6deqQsdB+esBwcSUEAIiGIgQAiIYiBACIhiIEAIiGYEIBWTenrQ3CrON9m8lZbV6scSuw4GvbY93gtuZn3eAP2ezNuvFtBRastj3W8b7XxWrDExJAsJw5cybo+Ouuu847/s4773jHfYGF0FCK9X76hG6iCAwXV0IAgGgoQgCAaChCAIBoKEIAgGgoQgCAaC4rHdfW1qYnnnhCq1ev1oYNGyR9kJx66qmntGnTJvX29qq+vl7PPPOMZs+eXYj5jklW0shq0eJLglnpKysdZyWkfKxUm5WECm3n40vfWckuKwVntb8JSXxZr7fVKujYsWOXPBcr7We991ZSb9KkSd5xH1+KUrLfn2Qy6R33zZEUHK6UYV8J7dmzR5s2bdLcuXMHja9bt07r16/Xxo0btWfPHqXTaS1evNj8SwcAuHoNqwidOnVKDz30kDZv3jxoS2vnnDZs2KC1a9dq6dKlmjNnjrZs2aIzZ85o69atBZs0AKA4DKsIPfzww7r//vv1mc98ZtB4V1eXMpmMGhsb82OJREILFy7U7t27vefK5XLq6+sb9AAAXB2C7wlt27ZNv/zlL7Vnz54hz2UyGUlSKpUaNJ5KpfTWW295z9fW1qannnoqdBoAgCIQdCXU3d2t1atX6/vf//5H3kC9+Gaxc868gdzc3KxsNpt/dHd3h0wJADCGBV0J7d27Vz09PZo3b15+7Ny5c9q1a5c2btyoQ4cOSfrgiujD/bh6enqGXB1dkEgkin4DLSutZPU98xXskA3jpA9ec5+ZM2cOGTt16pT3WCvZZSXVrB9MfJvm+TZSk+y0W29vr3e8vLzcO+4LwlhJQit5aCXEfKk067Wy3rfQVKPveyi0/551vO9rWsk7670HhivoO+q+++7T/v37tW/fvvxj/vz5euihh7Rv3z594hOfUDqdVkdHR/7PDAwMqLOzUw0NDQWfPABgbAu6EiovL9ecOXMGjZWVlWn69On58aamJrW2tqqurk51dXVqbW1VaWmpli1bVrhZAwCKQsG3clizZo3Onj2rVatW5T+sunPnTvPXJgCAq9dlF6GXX3550P+XlJSopaVFLS0tl3tqAECR4y4jACAadlYtIKsfmtWzy+JLk1msVFZlZaV3/PXXXx8yZu0s+uFuGB9mrdNK2flSZidPnvQeGyqkHZSVVLM+IG2t05cCtN4zK01mrd86TyF6uVk9Aq2ddYErgSshAEA0FCEAQDQUIQBANBQhAEA0FCEAQDTEYgqoUCmjkF1RrX5g1i6n6XR6yJjV285K3lnrtPqhWefxsZKEx48f945bO7H61m+l4EJ7F4akF63EYFlZmXfcSsGdPn16yJiV9rO+f+j7htGI70oAQDQUIQBANBQhAEA0FCEAQDQEEwrI2pDNd1NZsm8gWzf4faybzdls1jvuu5E/ZcqUoHlYYQAr4OBj3dy3NsazbvBbc/dtyma1IbIcPXrUO+4LT1ivlRWGsL4nLL7AgvV6hwYQfK+t9boChcaVEAAgGooQACAaihAAIBqKEAAgGooQACAa0nFXQGibG5/e3l7veHl5uXc8ZCO90M3OrKSalcryncdKwR07dsw7PmPGDO94LpfzjvvOH5Lek+zN/t58880hYzNnzgw6dyqV8o5bG+mFtHKyWO+zL0kIXClcCQEAoqEIAQCioQgBAKKhCAEAoqEIAQCiIR13BRRiszur75m1eZ3Fl7IL7alm9RWzkmq+FKCVVAtJ9Un2hnS+uZw5c8Z7rLU5nMWX1AtN3vX09HjHrRScr9ec9XpPmzbNO259H1ZUVAwZG8mUHvBhXAkBAKKhCAEAoqEIAQCioQgBAKKhCAEAoiEdV0BWDy5rJ9JCGBgY8I77duKU/Omz0J5iVs87K33lO4/VZ86aS2gqyzdHa97WOkN2KA3dPbeystI7bu04W1ZWdkljkp3Us+boSzuGpv2A4eJKCAAQDUUIABANRQgAEA1FCAAQDcGEAgq5kV0oVgDB4ptjoeZdiABG6HoKoRDrt0ICoazN/kKUlJR4x612Sz4hGy4Cl4MrIQBANBQhAEA0FCEAQDQUIQBANBQhAEA0FCEAQDQUIQBANBQhAEA0FCEAQDQUIQBANBQhAEA0QUWopaVFJSUlgx7pdDr/vHNOLS0tqq6u1uTJk7Vo0SIdOHCg4JMGABSH4Cuh2bNn6+jRo/nH/v3788+tW7dO69ev18aNG7Vnzx6l02ktXrxY/f39BZ00AKA4BHfRHj9+/KCrnwucc9qwYYPWrl2rpUuXSpK2bNmiVCqlrVu3asWKFd7z5XI55XK5/P/39fWFTgkAMEYFXwkdPnxY1dXVqq2t1Re+8AUdOXJEktTV1aVMJqPGxsb8sYlEQgsXLtTu3bvN87W1tSmZTOYfNTU1w1gGAGAsCipC9fX1eu655/TSSy9p8+bNymQyamho0PHjx5XJZCRJqVRq0J9JpVL553yam5uVzWbzj+7u7mEsAwAwFgX9Om7JkiX5/77tttu0YMEC3XDDDdqyZYvuvvtuSUM31HLOmZtsSR9cLSUSiZBpAACKxGVFtMvKynTbbbfp8OHD+ftEF1/19PT0DLk6AgBAuswilMvl9Otf/1pVVVWqra1VOp1WR0dH/vmBgQF1dnaqoaHhsicKACg+Qb+O+4d/+Ac98MADuv7669XT06NvfvOb6uvr0/Lly1VSUqKmpia1traqrq5OdXV1am1tVWlpqZYtWzZS8wcAjGFBReidd97RF7/4RR07dkwzZ87U3XffrVdeeUWzZs2SJK1Zs0Znz57VqlWr1Nvbq/r6eu3cuVPl5eUjMnkAwNhW4pxzsSfxYX19fUomk8pms6qoqIg9HQBAoJB/x+kdBwCIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIJrgI/fa3v9WXvvQlTZ8+XaWlpfrUpz6lvXv35p93zqmlpUXV1dWaPHmyFi1apAMHDhR00gCA4hBUhHp7e3XPPfdowoQJ+slPfqKDBw/qn//5nzV16tT8MevWrdP69eu1ceNG7dmzR+l0WosXL1Z/f3+h5w4AGONKnHPuUg9+/PHH9b//+7/6+c9/7n3eOafq6mo1NTXpsccekyTlcjmlUin90z/9k1asWPGxX6Ovr0/JZFLZbFYVFRWXOjUAwCgR8u940JXQjh07NH/+fH3uc59TZWWl7rjjDm3evDn/fFdXlzKZjBobG/NjiURCCxcu1O7du73nzOVy6uvrG/QAAFwdgorQkSNH1N7errq6Or300ktauXKlvv71r+u5556TJGUyGUlSKpUa9OdSqVT+uYu1tbUpmUzmHzU1NcNZBwBgDAoqQufPn9edd96p1tZW3XHHHVqxYoX+9m//Vu3t7YOOKykpGfT/zrkhYxc0Nzcrm83mH93d3YFLAACMVUFFqKqqSrfeeuugsVtuuUVvv/22JCmdTkvSkKuenp6eIVdHFyQSCVVUVAx6AACuDkFF6J577tGhQ4cGjb3xxhuaNWuWJKm2tlbpdFodHR355wcGBtTZ2amGhoYCTBcAUEzGhxz8d3/3d2poaFBra6v+6q/+Sr/4xS+0adMmbdq0SdIHv4ZrampSa2ur6urqVFdXp9bWVpWWlmrZsmUjsgAAwNgVVITuuusubd++Xc3NzfrGN76h2tpabdiwQQ899FD+mDVr1ujs2bNatWqVent7VV9fr507d6q8vLzgkwcAjG1BnxO6EvicEACMbSP2OSEAAAqJIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIJqiL9pVwoZ9qX19f5JkAAIbjwr/fl9Ife9QVof7+fklSTU1N5JkAAC5Hf3+/ksnkRx4z6rZyOH/+vH73u9+pvLxc/f39qqmpUXd3d1Fv69DX18c6i8jVsM6rYY0S6xwu55z6+/tVXV2tceM++q7PqLsSGjdunK677jpJH+zUKkkVFRVF/Q1wAessLlfDOq+GNUqsczg+7groAoIJAIBoKEIAgGhGdRFKJBJ68sknlUgkYk9lRLHO4nI1rPNqWKPEOq+EURdMAABcPUb1lRAAoLhRhAAA0VCEAADRUIQAANFQhAAA0YzqIvTd735XtbW1mjRpkubNm6ef//znsad0WXbt2qUHHnhA1dXVKikp0Q9+8INBzzvn1NLSourqak2ePFmLFi3SgQMH4kx2mNra2nTXXXepvLxclZWVevDBB3Xo0KFBxxTDOtvb2zV37tz8J8wXLFign/zkJ/nni2GNF2tra1NJSYmampryY8WwzpaWFpWUlAx6pNPp/PPFsMYLfvvb3+pLX/qSpk+frtLSUn3qU5/S3r17889HWasbpbZt2+YmTJjgNm/e7A4ePOhWr17tysrK3FtvvRV7asP24x//2K1du9Y9//zzTpLbvn37oOeffvppV15e7p5//nm3f/9+9/nPf95VVVW5vr6+OBMehj//8z93zz77rPvVr37l9u3b5+6//353/fXXu1OnTuWPKYZ17tixw/3Xf/2XO3TokDt06JB74okn3IQJE9yvfvUr51xxrPHDfvGLX7g/+qM/cnPnznWrV6/OjxfDOp988kk3e/Zsd/To0fyjp6cn/3wxrNE5506cOOFmzZrlvvKVr7j/+7//c11dXe6///u/3Ztvvpk/JsZaR20R+uM//mO3cuXKQWOf/OQn3eOPPx5pRoV1cRE6f/68S6fT7umnn86Pvffeey6ZTLp//dd/jTDDwujp6XGSXGdnp3OueNfpnHPXXnut+7d/+7eiW2N/f7+rq6tzHR0dbuHChfkiVCzrfPLJJ93tt9/ufa5Y1uicc4899pi79957zedjrXVU/jpuYGBAe/fuVWNj46DxxsZG7d69O9KsRlZXV5cymcygNScSCS1cuHBMrzmbzUqSpk2bJqk413nu3Dlt27ZNp0+f1oIFC4pujQ8//LDuv/9+feYznxk0XkzrPHz4sKqrq1VbW6svfOELOnLkiKTiWuOOHTs0f/58fe5zn1NlZaXuuOMObd68Of98rLWOyiJ07NgxnTt3TqlUatB4KpVSJpOJNKuRdWFdxbRm55weffRR3XvvvZozZ46k4lrn/v37NWXKFCUSCa1cuVLbt2/XrbfeWlRr3LZtm375y1+qra1tyHPFss76+no999xzeumll7R582ZlMhk1NDTo+PHjRbNGSTpy5Ija29tVV1enl156SStXrtTXv/51Pffcc5LivZ+jbiuHD7uwlcMFzrkhY8WmmNb8yCOP6PXXX9f//M//DHmuGNZ58803a9++fTp58qSef/55LV++XJ2dnfnnx/oau7u7tXr1au3cuVOTJk0yjxvr61yyZEn+v2+77TYtWLBAN9xwg7Zs2aK7775b0thfo/TBXm3z589Xa2urJOmOO+7QgQMH1N7err/+67/OH3el1zoqr4RmzJiha665Zkj17enpGVKli8WFNE6xrPlrX/uaduzYoZ/97Gf5/aGk4lrnxIkTdeONN2r+/Plqa2vT7bffru985ztFs8a9e/eqp6dH8+bN0/jx4zV+/Hh1dnbqX/7lXzR+/Pj8Wsb6Oi9WVlam2267TYcPHy6a91KSqqqqdOuttw4au+WWW/T2229Livd3c1QWoYkTJ2revHnq6OgYNN7R0aGGhoZIsxpZtbW1SqfTg9Y8MDCgzs7OMbVm55weeeQRvfDCC/rpT3+q2traQc8Xyzp9nHPK5XJFs8b77rtP+/fv1759+/KP+fPn66GHHtK+ffv0iU98oijWebFcLqdf//rXqqqqKpr3UpLuueeeIR+XeOONNzRr1ixJEf9ujljk4TJdiGj/+7//uzt48KBrampyZWVl7je/+U3sqQ1bf3+/e+2119xrr73mJLn169e71157LR87f/rpp10ymXQvvPCC279/v/viF7845qKgX/3qV10ymXQvv/zyoMjrmTNn8scUwzqbm5vdrl27XFdXl3v99dfdE0884caNG+d27tzpnCuONfp8OB3nXHGs8+///u/dyy+/7I4cOeJeeeUV95d/+ZeuvLw8/29NMazRuQ9i9uPHj3ff+ta33OHDh91//Md/uNLSUvf9738/f0yMtY7aIuScc88884ybNWuWmzhxorvzzjvzMd+x6mc/+5mTNOSxfPly59wHEcknn3zSpdNpl0gk3Kc//Wm3f//+uJMO5FufJPfss8/mjymGdf7N3/xN/ntz5syZ7r777ssXIOeKY40+FxehYljnhc/CTJgwwVVXV7ulS5e6AwcO5J8vhjVe8KMf/cjNmTPHJRIJ98lPftJt2rRp0PMx1sp+QgCAaEblPSEAwNWBIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiIYiBACIhiIEAIiGIgQAiOb/Aanm+UFV0EFcAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a3611ce",
   "metadata": {
    "id": "8a3611ce"
   },
   "outputs": [],
   "source": [
    "test_image=img_to_array(test_image)\n",
    "test_image=np.expand_dims(test_image,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10bd2180",
   "metadata": {
    "id": "10bd2180"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8954e14",
   "metadata": {
    "id": "e8954e14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'अ': 0,\n",
       " 'अं': 1,\n",
       " 'अः': 2,\n",
       " 'आ': 3,\n",
       " 'इ': 4,\n",
       " 'ई': 5,\n",
       " 'उ': 6,\n",
       " 'ऊ': 7,\n",
       " 'ऋ': 8,\n",
       " 'ए': 9,\n",
       " 'ऐ': 10,\n",
       " 'ओ': 11,\n",
       " 'औ': 12,\n",
       " 'क': 13,\n",
       " 'कं': 14,\n",
       " 'कः': 15,\n",
       " 'का': 16,\n",
       " 'की': 17,\n",
       " 'कु': 18,\n",
       " 'कू': 19,\n",
       " 'के': 20,\n",
       " 'कै': 21,\n",
       " 'को': 22,\n",
       " 'कौ': 23,\n",
       " 'क्क': 24,\n",
       " 'क्त': 25,\n",
       " 'क्र': 26,\n",
       " 'क्ल': 27,\n",
       " 'क्ल्य': 28,\n",
       " 'क्ष': 29,\n",
       " 'क्षं_': 30,\n",
       " 'क्षः': 31,\n",
       " 'क्षा_': 32,\n",
       " 'क्षि_': 33,\n",
       " 'क्षी_': 34,\n",
       " 'क्षु_': 35,\n",
       " 'क्षू_': 36,\n",
       " 'क्षे_': 37,\n",
       " 'क्षै_': 38,\n",
       " 'क्षो_': 39,\n",
       " 'क्षौ_': 40,\n",
       " 'क्ष्य': 41,\n",
       " 'ख': 42,\n",
       " 'खं': 43,\n",
       " 'खः': 44,\n",
       " 'खा': 45,\n",
       " 'खि': 46,\n",
       " 'खी': 47,\n",
       " 'खु': 48,\n",
       " 'खू': 49,\n",
       " 'खे': 50,\n",
       " 'खै': 51,\n",
       " 'खो': 52,\n",
       " 'खौ': 53,\n",
       " 'ख्य': 54,\n",
       " 'ग': 55,\n",
       " 'गं': 56,\n",
       " 'गः': 57,\n",
       " 'गा': 58,\n",
       " 'गि': 59,\n",
       " 'गी': 60,\n",
       " 'गु': 61,\n",
       " 'गू': 62,\n",
       " 'गे': 63,\n",
       " 'गै': 64,\n",
       " 'गो': 65,\n",
       " 'गौ': 66,\n",
       " 'ग्ग': 67,\n",
       " 'ग्य': 68,\n",
       " 'ग्र': 69,\n",
       " 'ग्ल': 70,\n",
       " 'घ': 71,\n",
       " 'घं': 72,\n",
       " 'घः': 73,\n",
       " 'घा': 74,\n",
       " 'घि': 75,\n",
       " 'घी': 76,\n",
       " 'घु': 77,\n",
       " 'घू': 78,\n",
       " 'घे': 79,\n",
       " 'घै': 80,\n",
       " 'घो': 81,\n",
       " 'घौ': 82,\n",
       " 'ङ': 83,\n",
       " 'च': 84,\n",
       " 'चं': 85,\n",
       " 'चः': 86,\n",
       " 'चा': 87,\n",
       " 'चि': 88,\n",
       " 'ची': 89,\n",
       " 'चु': 90,\n",
       " 'चू': 91,\n",
       " 'चे': 92,\n",
       " 'चै': 93,\n",
       " 'चो': 94,\n",
       " 'चौ': 95,\n",
       " 'च्न': 96,\n",
       " 'च्य': 97,\n",
       " 'च्ल': 98,\n",
       " 'छ': 99,\n",
       " 'छं': 100,\n",
       " 'छः': 101,\n",
       " 'छा': 102,\n",
       " 'छि': 103,\n",
       " 'छी': 104,\n",
       " 'छु': 105,\n",
       " 'छू': 106,\n",
       " 'छे': 107,\n",
       " 'छै': 108,\n",
       " 'छो': 109,\n",
       " 'छौ': 110,\n",
       " 'छ्य': 111,\n",
       " 'ज': 112,\n",
       " 'जं': 113,\n",
       " 'जः': 114,\n",
       " 'जा': 115,\n",
       " 'जि': 116,\n",
       " 'जी': 117,\n",
       " 'जु': 118,\n",
       " 'जू': 119,\n",
       " 'जे': 120,\n",
       " 'जै': 121,\n",
       " 'जो': 122,\n",
       " 'जौ': 123,\n",
       " 'ज्ञ': 124,\n",
       " 'ज्ञं_': 125,\n",
       " 'ज्ञः': 126,\n",
       " 'ज्ञा_': 127,\n",
       " 'ज्ञि_': 128,\n",
       " 'ज्ञी': 129,\n",
       " 'ज्ञु_': 130,\n",
       " 'ज्ञू_': 131,\n",
       " 'ज्ञे_': 132,\n",
       " 'ज्ञै_': 133,\n",
       " 'ज्ञो_': 134,\n",
       " 'ज्ञौ_': 135,\n",
       " 'ज्य': 136,\n",
       " 'झ': 137,\n",
       " 'झं': 138,\n",
       " 'झः': 139,\n",
       " 'झा': 140,\n",
       " 'झि': 141,\n",
       " 'झी': 142,\n",
       " 'झु': 143,\n",
       " 'झू': 144,\n",
       " 'झे': 145,\n",
       " 'झै': 146,\n",
       " 'झो': 147,\n",
       " 'झौ': 148,\n",
       " 'ञ': 149,\n",
       " 'ट': 150,\n",
       " 'टं': 151,\n",
       " 'टः': 152,\n",
       " 'टा': 153,\n",
       " 'टि': 154,\n",
       " 'टी': 155,\n",
       " 'टु': 156,\n",
       " 'टू': 157,\n",
       " 'टे': 158,\n",
       " 'टै': 159,\n",
       " 'टो': 160,\n",
       " 'टौ': 161,\n",
       " 'ट्य': 162,\n",
       " 'ठ': 163,\n",
       " 'ठं': 164,\n",
       " 'ठः': 165,\n",
       " 'ठा': 166,\n",
       " 'ठि': 167,\n",
       " 'ठी': 168,\n",
       " 'ठु': 169,\n",
       " 'ठू': 170,\n",
       " 'ठे': 171,\n",
       " 'ठै': 172,\n",
       " 'ठो': 173,\n",
       " 'ठौ': 174,\n",
       " 'ड': 175,\n",
       " 'डं': 176,\n",
       " 'डः': 177,\n",
       " 'डा': 178,\n",
       " 'डि': 179,\n",
       " 'डी': 180,\n",
       " 'डु': 181,\n",
       " 'डू': 182,\n",
       " 'डे': 183,\n",
       " 'डै': 184,\n",
       " 'डो': 185,\n",
       " 'डौ': 186,\n",
       " 'ड्य': 187,\n",
       " 'ढ': 188,\n",
       " 'ढं': 189,\n",
       " 'ढः': 190,\n",
       " 'ढा': 191,\n",
       " 'ढि': 192,\n",
       " 'ढी': 193,\n",
       " 'ढु': 194,\n",
       " 'ढू': 195,\n",
       " 'ढे': 196,\n",
       " 'ढै': 197,\n",
       " 'ढो': 198,\n",
       " 'ढौ': 199,\n",
       " 'ढ्य': 200,\n",
       " 'ण': 201,\n",
       " 'णं': 202,\n",
       " 'णः': 203,\n",
       " 'णा': 204,\n",
       " 'णि': 205,\n",
       " 'णी': 206,\n",
       " 'णु': 207,\n",
       " 'णू': 208,\n",
       " 'णे': 209,\n",
       " 'णै': 210,\n",
       " 'णो': 211,\n",
       " 'णौ': 212,\n",
       " 'त': 213,\n",
       " 'तं': 214,\n",
       " 'तः': 215,\n",
       " 'ता': 216,\n",
       " 'ति': 217,\n",
       " 'ती': 218,\n",
       " 'तु': 219,\n",
       " 'तू': 220,\n",
       " 'ते': 221,\n",
       " 'तै': 222,\n",
       " 'तो': 223,\n",
       " 'तौ': 224,\n",
       " 'त्त': 225,\n",
       " 'त्त्व': 226,\n",
       " 'त्य': 227,\n",
       " 'त्र': 228,\n",
       " 'थ': 229,\n",
       " 'थं_': 230,\n",
       " 'थः': 231,\n",
       " 'था': 232,\n",
       " 'थि': 233,\n",
       " 'थी': 234,\n",
       " 'थु_': 235,\n",
       " 'थू_': 236,\n",
       " 'थे': 237,\n",
       " 'थै': 238,\n",
       " 'थो': 239,\n",
       " 'थौ': 240,\n",
       " 'द': 241,\n",
       " 'दं_': 242,\n",
       " 'दः': 243,\n",
       " 'दा': 244,\n",
       " 'दि_': 245,\n",
       " 'दी_': 246,\n",
       " 'दु_': 247,\n",
       " 'दू': 248,\n",
       " 'दे_': 249,\n",
       " 'दै_': 250,\n",
       " 'दो_': 251,\n",
       " 'दौ_': 252,\n",
       " 'द्द': 253,\n",
       " 'द्र': 254,\n",
       " 'ध': 255,\n",
       " 'धं_': 256,\n",
       " 'धः': 257,\n",
       " 'धा_': 258,\n",
       " 'धि_': 259,\n",
       " 'धी_': 260,\n",
       " 'धु_': 261,\n",
       " 'धू': 262,\n",
       " 'धे_': 263,\n",
       " 'धै_': 264,\n",
       " 'धो': 265,\n",
       " 'धौ_': 266,\n",
       " 'ध्य': 267,\n",
       " 'ध्य्य': 268,\n",
       " 'ध्र': 269,\n",
       " 'न': 270,\n",
       " 'नं_': 271,\n",
       " 'नः': 272,\n",
       " 'ना_': 273,\n",
       " 'नि_': 274,\n",
       " 'नी_': 275,\n",
       " 'नु_': 276,\n",
       " 'नू_': 277,\n",
       " 'ने_': 278,\n",
       " 'नै_': 279,\n",
       " 'नो_': 280,\n",
       " 'नौ_': 281,\n",
       " 'न्त': 282,\n",
       " 'न्त्य': 283,\n",
       " 'न्त्र': 284,\n",
       " 'न्ध': 285,\n",
       " 'न्ध्र': 286,\n",
       " 'न्न': 287,\n",
       " 'न्म': 288,\n",
       " 'न्य': 289,\n",
       " 'न्ल': 290,\n",
       " 'प': 291,\n",
       " 'पं_': 292,\n",
       " 'पः': 293,\n",
       " 'पा': 294,\n",
       " 'पि_': 295,\n",
       " 'पी_': 296,\n",
       " 'पु': 297,\n",
       " 'पू_': 298,\n",
       " 'पे_': 299,\n",
       " 'पै_': 300,\n",
       " 'पो_': 301,\n",
       " 'पौ_': 302,\n",
       " 'प्त्य': 303,\n",
       " 'प्न': 304,\n",
       " 'प्प': 305,\n",
       " 'प्य': 306,\n",
       " 'प्र': 307,\n",
       " 'प्ल': 308,\n",
       " 'फ': 309,\n",
       " 'फं_': 310,\n",
       " 'फः': 311,\n",
       " 'फा_': 312,\n",
       " 'फि': 313,\n",
       " 'फी_': 314,\n",
       " 'फु': 315,\n",
       " 'फू_': 316,\n",
       " 'फे_': 317,\n",
       " 'फै_': 318,\n",
       " 'फो_': 319,\n",
       " 'फौ_': 320,\n",
       " 'फ्य': 321,\n",
       " 'ब': 322,\n",
       " 'बं_': 323,\n",
       " 'बः': 324,\n",
       " 'बा': 325,\n",
       " 'बि': 326,\n",
       " 'बी': 327,\n",
       " 'बु': 328,\n",
       " 'बू': 329,\n",
       " 'बे_': 330,\n",
       " 'बै_': 331,\n",
       " 'बो_': 332,\n",
       " 'बौ_': 333,\n",
       " 'ब्ब': 334,\n",
       " 'ब्य': 335,\n",
       " 'भ': 336,\n",
       " 'भं_': 337,\n",
       " 'भः': 338,\n",
       " 'भा': 339,\n",
       " 'भि': 340,\n",
       " 'भी': 341,\n",
       " 'भु_': 342,\n",
       " 'भू_': 343,\n",
       " 'भे_': 344,\n",
       " 'भै_': 345,\n",
       " 'भो_': 346,\n",
       " 'भौ_': 347,\n",
       " 'म': 348,\n",
       " 'मं': 349,\n",
       " 'मः': 350,\n",
       " 'मा_': 351,\n",
       " 'मि_': 352,\n",
       " 'मी_': 353,\n",
       " 'मु_': 354,\n",
       " 'मू_': 355,\n",
       " 'मे': 356,\n",
       " 'मै_': 357,\n",
       " 'मो_': 358,\n",
       " 'मौ_': 359,\n",
       " 'म्त': 360,\n",
       " 'म्म': 361,\n",
       " 'म्य': 362,\n",
       " 'म्ल': 363,\n",
       " 'य': 364,\n",
       " 'यं': 365,\n",
       " 'यः': 366,\n",
       " 'या': 367,\n",
       " 'यि': 368,\n",
       " 'यी': 369,\n",
       " 'यु': 370,\n",
       " 'यू': 371,\n",
       " 'ये': 372,\n",
       " 'यै': 373,\n",
       " 'यो': 374,\n",
       " 'यौ': 375,\n",
       " 'य्त': 376,\n",
       " 'य्य': 377,\n",
       " 'य्व': 378,\n",
       " 'र': 379,\n",
       " 'रं': 380,\n",
       " 'रः': 381,\n",
       " 'रा': 382,\n",
       " 'रि': 383,\n",
       " 'री': 384,\n",
       " 'रु': 385,\n",
       " 'रू': 386,\n",
       " 'रे': 387,\n",
       " 'रै': 388,\n",
       " 'रो': 389,\n",
       " 'रौ': 390,\n",
       " 'र्ख': 391,\n",
       " 'र्ग': 392,\n",
       " 'र्घ': 393,\n",
       " 'र्ण': 394,\n",
       " 'र्म': 395,\n",
       " 'र्य': 396,\n",
       " 'र्ल': 397,\n",
       " 'र्व': 398,\n",
       " 'ल': 399,\n",
       " 'लं_': 400,\n",
       " 'लः': 401,\n",
       " 'ला_': 402,\n",
       " 'लि_': 403,\n",
       " 'ली_': 404,\n",
       " 'लु_': 405,\n",
       " 'लू_': 406,\n",
       " 'ले_': 407,\n",
       " 'लै_': 408,\n",
       " 'लो_': 409,\n",
       " 'लौ_': 410,\n",
       " 'ल्ग': 411,\n",
       " 'ल्घ': 412,\n",
       " 'ल्त': 413,\n",
       " 'ल्म': 414,\n",
       " 'ल्य': 415,\n",
       " 'ल्य्य': 416,\n",
       " 'ल्ल': 417,\n",
       " 'ळ': 418,\n",
       " 'ळं': 419,\n",
       " 'ळः': 420,\n",
       " 'ळा': 421,\n",
       " 'ळि': 422,\n",
       " 'ळी': 423,\n",
       " 'ळु': 424,\n",
       " 'ळू': 425,\n",
       " 'ळे': 426,\n",
       " 'ळै': 427,\n",
       " 'ळो': 428,\n",
       " 'ळौ': 429,\n",
       " 'व': 430,\n",
       " 'वं_': 431,\n",
       " 'वः': 432,\n",
       " 'वा_': 433,\n",
       " 'वि_': 434,\n",
       " 'वी': 435,\n",
       " 'वु_': 436,\n",
       " 'वू_': 437,\n",
       " 'वे_': 438,\n",
       " 'वै_': 439,\n",
       " 'वो_': 440,\n",
       " 'वौ_': 441,\n",
       " 'व्त': 442,\n",
       " 'व्य': 443,\n",
       " 'व्य्य': 444,\n",
       " 'व्र': 445,\n",
       " 'व्ल': 446,\n",
       " 'व्व': 447,\n",
       " 'श': 448,\n",
       " 'शं_': 449,\n",
       " 'शः': 450,\n",
       " 'शा_': 451,\n",
       " 'शि_': 452,\n",
       " 'शी_': 453,\n",
       " 'शु_': 454,\n",
       " 'शू_': 455,\n",
       " 'शे_': 456,\n",
       " 'शै_': 457,\n",
       " 'शो': 458,\n",
       " 'शौ_': 459,\n",
       " 'श्च': 460,\n",
       " 'श्छ': 461,\n",
       " 'श्त': 462,\n",
       " 'श्म': 463,\n",
       " 'श्र': 464,\n",
       " 'श्ल्य': 465,\n",
       " 'श्व': 466,\n",
       " 'ष': 467,\n",
       " 'षं_': 468,\n",
       " 'षः': 469,\n",
       " 'षा': 470,\n",
       " 'षि_': 471,\n",
       " 'षी_': 472,\n",
       " 'षु': 473,\n",
       " 'षू_': 474,\n",
       " 'षे_': 475,\n",
       " 'षै_': 476,\n",
       " 'षो_': 477,\n",
       " 'षौ_': 478,\n",
       " 'ष्क': 479,\n",
       " 'ष्ट': 480,\n",
       " 'ष्ट्र': 481,\n",
       " 'ष्ठ': 482,\n",
       " 'ष्य': 483,\n",
       " 'स': 484,\n",
       " 'सं_': 485,\n",
       " 'सः': 486,\n",
       " 'सा_': 487,\n",
       " 'सी_': 488,\n",
       " 'सु_': 489,\n",
       " 'सू_': 490,\n",
       " 'से_': 491,\n",
       " 'सै': 492,\n",
       " 'सो': 493,\n",
       " 'सौ_': 494,\n",
       " 'स्त': 495,\n",
       " 'स्म': 496,\n",
       " 'स्य': 497,\n",
       " 'स्ल': 498,\n",
       " 'स्स': 499,\n",
       " 'ह': 500,\n",
       " 'हं_': 501,\n",
       " 'हः': 502,\n",
       " 'हा_': 503,\n",
       " 'हि_': 504,\n",
       " 'ही_': 505,\n",
       " 'हु_': 506,\n",
       " 'हू_': 507,\n",
       " 'हे_': 508,\n",
       " 'है_': 509,\n",
       " 'हो_': 510,\n",
       " 'हौ_': 511,\n",
       " 'ह्य': 512}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7807a4a6",
   "metadata": {
    "id": "7807a4a6"
   },
   "outputs": [],
   "source": [
    "indices = training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0405012c",
   "metadata": {
    "id": "0405012c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ढी\n"
     ]
    }
   ],
   "source": [
    "result = np.argmax(model.predict(test_image,verbose=False))\n",
    "print(list(indices.keys())[list(indices.values()).index(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc236e",
   "metadata": {
    "id": "b1dc236e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
